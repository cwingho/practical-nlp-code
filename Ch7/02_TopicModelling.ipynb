{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "This notebook contains a demo of LDA and LSA using the gensim library. The dataset's link can be found in the `BookSummaries_Link.md` file under the Data folder in Ch7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\sukee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\sukee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (1.6.3)\n",
      "Requirement already satisfied: Cython==0.29.21 in c:\\users\\sukee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (0.29.21)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\sukee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (5.0.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\sukee\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from gensim) (1.20.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\sukee\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Import OS \n",
    "import os\n",
    "# For NLTK virtual environments are high recommended and it requires python verisions higher than 3.5\n",
    "!pip install gensim\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/brian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from pprint import pprint\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6f6531c52acc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Please update it to your actual download path regradless of your choice of operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msummaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "#tokenize, remove stopwords, non-alphabetic words, lowercase\n",
    "def preprocess(textstring):\n",
    "   stops =  set(stopwords.words('english'))\n",
    "   tokens = word_tokenize(textstring)\n",
    "   return [token.lower() for token in tokens if token.isalpha() and token not in stops]\n",
    "\n",
    "# This is a sample path of your downloaded data set. This is currently set to a windows based path . \n",
    "# Please update it to your actual download path regradless of your choice of operating system \n",
    "\n",
    "data_path = os.path.join(os.path.dirname(os.path.abspath(__file__)),data)\n",
    "\n",
    "summaries = []\n",
    "for line in open(data_path, encoding=\"utf-8\"):\n",
    "   temp = line.split(\"\\t\")\n",
    "   summaries.append(preprocess(temp[6]))\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "\n",
    "dictionary = Dictionary(summaries)\n",
    "\n",
    "# Filter infrequent or too frequent words.\n",
    "\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(summary) for summary in summaries]\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "#Train the topic model\n",
    "\n",
    "model = LdaModel(corpus=corpus, id2word=id2word,iterations=400, num_topics=10)\n",
    "top_topics = list(model.top_topics(corpus))\n",
    "pprint(top_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: 0.013*\"jacky\" + 0.006*\"dahlia\" + 0.005*\"novel\" + 0.005*\"one\" + 0.004*\"story\" + 0.004*\"also\" + 0.004*\"book\" + 0.004*\"team\" + 0.004*\"narrator\" + 0.003*\"jeremy\"\n",
      "Topic #1: 0.010*\"book\" + 0.009*\"war\" + 0.006*\"in\" + 0.006*\"world\" + 0.005*\"novel\" + 0.005*\"states\" + 0.004*\"also\" + 0.004*\"new\" + 0.004*\"chapter\" + 0.004*\"story\"\n",
      "Topic #2: 0.008*\"he\" + 0.007*\"she\" + 0.006*\"mother\" + 0.006*\"one\" + 0.005*\"tells\" + 0.005*\"back\" + 0.005*\"house\" + 0.005*\"father\" + 0.005*\"school\" + 0.005*\"go\"\n",
      "Topic #3: 0.007*\"life\" + 0.006*\"love\" + 0.006*\"family\" + 0.006*\"father\" + 0.006*\"he\" + 0.006*\"novel\" + 0.005*\"young\" + 0.005*\"she\" + 0.004*\"story\" + 0.004*\"one\"\n",
      "Topic #4: 0.007*\"he\" + 0.006*\"one\" + 0.004*\"murder\" + 0.004*\"police\" + 0.004*\"man\" + 0.003*\"two\" + 0.003*\"case\" + 0.003*\"also\" + 0.003*\"would\" + 0.003*\"time\"\n",
      "Topic #5: 0.007*\"earth\" + 0.006*\"one\" + 0.005*\"time\" + 0.005*\"human\" + 0.005*\"world\" + 0.004*\"new\" + 0.004*\"planet\" + 0.004*\"life\" + 0.003*\"space\" + 0.003*\"he\"\n",
      "Topic #6: 0.006*\"he\" + 0.005*\"they\" + 0.005*\"one\" + 0.004*\"back\" + 0.004*\"find\" + 0.004*\"ship\" + 0.004*\"king\" + 0.003*\"two\" + 0.003*\"city\" + 0.003*\"help\"\n",
      "Topic #7: 0.006*\"alex\" + 0.005*\"one\" + 0.005*\"henry\" + 0.004*\"freddy\" + 0.004*\"time\" + 0.004*\"simon\" + 0.004*\"he\" + 0.004*\"luce\" + 0.004*\"new\" + 0.004*\"find\"\n",
      "Topic #8: 0.006*\"will\" + 0.006*\"jason\" + 0.005*\"he\" + 0.005*\"vampire\" + 0.005*\"king\" + 0.004*\"leo\" + 0.004*\"one\" + 0.004*\"new\" + 0.004*\"in\" + 0.003*\"world\"\n",
      "Topic #9: 0.005*\"jake\" + 0.005*\"charlie\" + 0.005*\"she\" + 0.004*\"he\" + 0.004*\"one\" + 0.004*\"roger\" + 0.004*\"back\" + 0.004*\"they\" + 0.004*\"luke\" + 0.004*\"new\"\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(\"Topic #%s:\" % idx, model.print_topic(idx, 10))\n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.305*\"he\" + 0.215*\"one\" + 0.150*\"she\" + 0.140*\"time\" + 0.132*\"back\" + '\n",
      "  '0.131*\"also\" + 0.127*\"two\" + 0.125*\"they\" + 0.123*\"tells\" + 0.118*\"in\"'),\n",
      " (1,\n",
      "  '0.493*\"tom\" + 0.226*\"sophia\" + 0.182*\"mrs\" + 0.178*\"house\" + 0.161*\"she\" + '\n",
      "  '0.154*\"father\" + 0.147*\"mr\" + 0.146*\"he\" + 0.138*\"tells\" + -0.126*\"one\"'),\n",
      " (2,\n",
      "  '-0.558*\"tom\" + -0.252*\"sophia\" + 0.213*\"she\" + 0.190*\"he\" + -0.185*\"mrs\" + '\n",
      "  '0.163*\"tells\" + 0.144*\"mother\" + -0.138*\"mr\" + -0.129*\"western\" + '\n",
      "  '-0.102*\"however\"'),\n",
      " (3,\n",
      "  '-0.233*\"they\" + -0.203*\"ship\" + 0.187*\"he\" + -0.183*\"david\" + -0.178*\"back\" '\n",
      "  '+ -0.165*\"tells\" + 0.161*\"life\" + 0.160*\"family\" + 0.154*\"narrator\" + '\n",
      "  '-0.154*\"find\"'),\n",
      " (4,\n",
      "  '0.664*\"he\" + -0.258*\"mother\" + -0.213*\"she\" + -0.195*\"father\" + '\n",
      "  '-0.180*\"family\" + 0.121*\"narrator\" + 0.120*\"monk\" + -0.100*\"school\" + '\n",
      "  '-0.099*\"novel\" + -0.095*\"children\"'),\n",
      " (5,\n",
      "  '0.486*\"david\" + -0.241*\"king\" + 0.169*\"rosa\" + 0.162*\"book\" + '\n",
      "  '0.126*\"harlan\" + -0.120*\"he\" + 0.111*\"she\" + 0.108*\"gould\" + -0.108*\"anita\" '\n",
      "  '+ 0.103*\"would\"'),\n",
      " (6,\n",
      "  '-0.698*\"anita\" + -0.471*\"richard\" + 0.155*\"ship\" + 0.133*\"jacky\" + '\n",
      "  '-0.085*\"edward\" + -0.084*\"power\" + -0.078*\"monk\" + 0.073*\"father\" + '\n",
      "  '-0.071*\"scene\" + -0.070*\"kill\"'),\n",
      " (7,\n",
      "  '-0.397*\"david\" + -0.357*\"king\" + 0.221*\"jacky\" + 0.190*\"ship\" + '\n",
      "  '0.145*\"monk\" + 0.134*\"doctor\" + -0.130*\"rosa\" + -0.121*\"prince\" + '\n",
      "  '-0.118*\"arthur\" + -0.109*\"book\"'),\n",
      " (8,\n",
      "  '-0.288*\"harry\" + 0.283*\"she\" + -0.261*\"narrator\" + 0.228*\"jacky\" + '\n",
      "  '-0.224*\"david\" + -0.195*\"monk\" + -0.143*\"natalie\" + 0.130*\"ship\" + '\n",
      "  '0.123*\"says\" + 0.122*\"king\"'),\n",
      " (9,\n",
      "  '0.451*\"harry\" + -0.411*\"narrator\" + 0.261*\"monk\" + -0.221*\"david\" + '\n",
      "  '-0.215*\"anita\" + 0.182*\"natalie\" + -0.168*\"ship\" + -0.163*\"he\" + '\n",
      "  '-0.153*\"richard\" + 0.152*\"dresden\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LsiModel\n",
    "lsamodel = LsiModel(corpus, num_topics=10, id2word = id2word)  # train model\n",
    "\n",
    "pprint(lsamodel.print_topics(num_topics=10, num_words=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: 0.305*\"he\" + 0.215*\"one\" + 0.150*\"she\" + 0.140*\"time\" + 0.132*\"back\" + 0.131*\"also\" + 0.127*\"two\" + 0.125*\"they\" + 0.123*\"tells\" + 0.118*\"in\"\n",
      "Topic #1: 0.493*\"tom\" + 0.226*\"sophia\" + 0.182*\"mrs\" + 0.178*\"house\" + 0.161*\"she\" + 0.154*\"father\" + 0.147*\"mr\" + 0.146*\"he\" + 0.138*\"tells\" + -0.126*\"one\"\n",
      "Topic #2: -0.558*\"tom\" + -0.252*\"sophia\" + 0.213*\"she\" + 0.190*\"he\" + -0.185*\"mrs\" + 0.163*\"tells\" + 0.144*\"mother\" + -0.138*\"mr\" + -0.129*\"western\" + -0.102*\"however\"\n",
      "Topic #3: -0.233*\"they\" + -0.203*\"ship\" + 0.187*\"he\" + -0.183*\"david\" + -0.178*\"back\" + -0.165*\"tells\" + 0.161*\"life\" + 0.160*\"family\" + 0.154*\"narrator\" + -0.154*\"find\"\n",
      "Topic #4: 0.664*\"he\" + -0.258*\"mother\" + -0.213*\"she\" + -0.195*\"father\" + -0.180*\"family\" + 0.121*\"narrator\" + 0.120*\"monk\" + -0.100*\"school\" + -0.099*\"novel\" + -0.095*\"children\"\n",
      "Topic #5: 0.486*\"david\" + -0.241*\"king\" + 0.169*\"rosa\" + 0.162*\"book\" + 0.126*\"harlan\" + -0.120*\"he\" + 0.111*\"she\" + 0.108*\"gould\" + -0.108*\"anita\" + 0.103*\"would\"\n",
      "Topic #6: -0.698*\"anita\" + -0.471*\"richard\" + 0.155*\"ship\" + 0.133*\"jacky\" + -0.085*\"edward\" + -0.084*\"power\" + -0.078*\"monk\" + 0.073*\"father\" + -0.071*\"scene\" + -0.070*\"kill\"\n",
      "Topic #7: -0.397*\"david\" + -0.357*\"king\" + 0.221*\"jacky\" + 0.190*\"ship\" + 0.145*\"monk\" + 0.134*\"doctor\" + -0.130*\"rosa\" + -0.121*\"prince\" + -0.118*\"arthur\" + -0.109*\"book\"\n",
      "Topic #8: -0.288*\"harry\" + 0.283*\"she\" + -0.261*\"narrator\" + 0.228*\"jacky\" + -0.224*\"david\" + -0.195*\"monk\" + -0.143*\"natalie\" + 0.130*\"ship\" + 0.123*\"says\" + 0.122*\"king\"\n",
      "Topic #9: 0.451*\"harry\" + -0.411*\"narrator\" + 0.261*\"monk\" + -0.221*\"david\" + -0.215*\"anita\" + 0.182*\"natalie\" + -0.168*\"ship\" + -0.163*\"he\" + -0.153*\"richard\" + 0.152*\"dresden\"\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(\"Topic #%s:\" % idx, lsamodel.print_topic(idx, 10))\n",
    " \n",
    "print(\"=\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
